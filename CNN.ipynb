{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/giacomobinco/FDS-Final-Project/blob/main/CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w3V_YMP3Va1j"
      },
      "outputs": [],
      "source": [
        "# Loading all the necessary libraries\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import seaborn as sns\n",
        "\n",
        "import random\n",
        "\n",
        "import zipfile\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import shutil\n",
        "import warnings\n",
        "\n",
        "from google.colab import files\n",
        "from google.colab import drive\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import random_split\n",
        "from torch.nn.functional import softmax\n",
        "\n",
        "from sklearn.model_selection import KFold, StratifiedShuffleSplit\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support, roc_curve, auc\n",
        "from sklearn.preprocessing import label_binarize"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Enabling the use of the new API settings\n",
        "torch.backends.cuda.matmul.fp32_precision = \"ieee\""
      ],
      "metadata": {
        "id": "CaktF1GlUJmo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking if the GPU is selected as hardware accelerator\n",
        "print(torch.cuda.is_available())\n",
        "print(torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")"
      ],
      "metadata": {
        "id": "MMTHOFV49WHS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If it returns *False, No GPU*, follow this path:\n",
        "\n",
        "Runtime > Cambia tipo di runtime > Acceleratore hardware > select *T4 GPU*"
      ],
      "metadata": {
        "id": "WeHFEnLt9e25"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define utility functions to control in the best way possible the reproducibility of future results\n",
        "# (A complete reproducibility is not guaranteed though)\n",
        "\n",
        "# Function 1\n",
        "\n",
        "def seed_everything(seed: int):\n",
        "\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# Function 2\n",
        "\n",
        "def seed_worker(worker_id):\n",
        "\n",
        "    worker_seed = torch.initial_seed() % 2**32\n",
        "    np.random.seed(worker_seed)\n",
        "    random.seed(worker_seed)"
      ],
      "metadata": {
        "id": "M1ZHTP29JG_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Executing these functions\n",
        "\n",
        "seed_everything(42);\n",
        "\n",
        "g = torch.Generator(); # \"g\" will be implemented in the \"DataLoader()\" functions ...\n",
        "g.manual_seed(42);     # ... together with the just defined function \"worker_seed()\""
      ],
      "metadata": {
        "id": "oohcgM9oJXCp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Uploading the zip folder (\"fma_spectrograms.zip\") with all the Mel-spectograms generated from the \"fma_small\" dataset\n",
        "# [ !! It takes about 25 minutes !! ]\n",
        "\n",
        "spectograms = files.upload()"
      ],
      "metadata": {
        "id": "ladpydHiVjNj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting all these images (spectrograms)\n",
        "\n",
        "zip_path = list(spectograms.keys())[0]\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/dataset')"
      ],
      "metadata": {
        "id": "hZvv769cWieX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the folder in which we want to save in an organized way the spectrograms\n",
        "output_directory = \"fma_spectrograms_organized\"\n",
        "os.makedirs(output_directory, exist_ok = True)\n",
        "\n",
        "# Open the zip folder previously loaded in \"reading\" (\"r\") modality\n",
        "with zipfile.ZipFile(\"fma_spectrograms.zip\", \"r\") as zip_ref:\n",
        "\n",
        "    # Gets the complete path of every file (.png in our case) in the folder\n",
        "    for member in zip_ref.namelist():\n",
        "\n",
        "        # Remove every sub-folder, if present\n",
        "        filename = os.path.basename(member)\n",
        "\n",
        "        # Skip the element if it's a folder and not a file\n",
        "        if filename:\n",
        "\n",
        "            # Open the file within the folder\n",
        "            source = zip_ref.open(member)\n",
        "\n",
        "            # Create the path in which the spectrogram (file) will be saved ...\n",
        "            target_path = os.path.join(output_directory, filename)\n",
        "\n",
        "            # ... and actually save it\n",
        "            with open(target_path, \"wb\") as target:\n",
        "                with source as src:\n",
        "                    target.write(src.read())"
      ],
      "metadata": {
        "id": "vLVli66Ora2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Connecting Google Drive to the present notebook ...\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "gGNvDIIbuA7d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ... to upload the metadata on the audio tracks\n",
        "tracks = pd.read_csv(r\"/content/drive/MyDrive/tracks.csv\", index_col = 0, header = [0,1])"
      ],
      "metadata": {
        "id": "sL3gcfo2uIIH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CNNs work better if the dataset is organized in folders, and that's what the next cell of code does.\n",
        "# The following structure is in fact expected:\n",
        "#\n",
        "# dataset/\n",
        "#    Rock/\n",
        "#        1234.png\n",
        "#        9876.png\n",
        "#        ...\n",
        "#    Classical/\n",
        "#        8765.png\n",
        "#        ...\n",
        "#    Hip-Hop/\n",
        "#        ...\n",
        "#    .../"
      ],
      "metadata": {
        "id": "SNRVlTNw7Rjg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# \"fma_spectrograms_organized\" (assigned to the variable \"output_directory\") is the folder created in the previous part of code,\n",
        "# as the name suggests, to save and organize all the spectrograms; whereas the final dataset will be saved in the following folder:\n",
        "dataset_directory = \"dataset\"\n",
        "os.makedirs(dataset_directory, exist_ok = True)\n",
        "\n",
        "# Counters\n",
        "copied = 0\n",
        "skipped = 0\n",
        "\n",
        "# For every file (spectrogram) saved in \"output_directory\" ...\n",
        "for fname in os.listdir(output_directory):\n",
        "\n",
        "    # ... consider only the .png files (there shouldn't be any others though)\n",
        "    if not fname.lower().endswith(\".png\"):\n",
        "        continue\n",
        "\n",
        "    # Get the audio track ID from the file name (according to the method used to generate them in the first place)\n",
        "    track_id_str = fname.split(\"_\")[0]\n",
        "\n",
        "    # Convert the file name (a sequence of numbers) in a \"int\" object\n",
        "    try:\n",
        "        track_id = int(track_id_str)\n",
        "\n",
        "    # It the conversion fails, the file is skipped\n",
        "    except:\n",
        "        skipped += 1\n",
        "        continue\n",
        "\n",
        "    # If the audio track ID doesn't exist in the audio tracks dataset (file \"tracks.csv\"), the file is skipped\n",
        "    if track_id not in tracks.index:\n",
        "        skipped += 1\n",
        "        continue\n",
        "\n",
        "    # From said dataset, collect the audio track's main genre\n",
        "    genre = tracks.loc[track_id, ('track','genre_top')]\n",
        "\n",
        "    # Create the genre folder (see the structure that the dataset should have)\n",
        "    genre_directory = os.path.join(dataset_directory, genre)\n",
        "    os.makedirs(genre_directory, exist_ok = True)\n",
        "\n",
        "    # Copy the spectrogram in the corresponding genre folder\n",
        "    shutil.copy(os.path.join(output_directory, fname), os.path.join(genre_directory, fname))\n",
        "    copied += 1\n",
        "\n",
        "# Visual check\n",
        "print(f\"✅ {copied} files has been copied in the genres' sub-folders\")\n",
        "if skipped > 0:\n",
        "  print(f\"\\n⚠️ {skipped} files with no correspondece has been found\")"
      ],
      "metadata": {
        "id": "JSI7Dme7ryod"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To remove a directory (two specific ones in this case):\n",
        "!rm -rf dataset/fma_spectrograms/\n",
        "!rm -rf sample_data/"
      ],
      "metadata": {
        "id": "zjkXXU4o5WkK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using the current structure of the dataset (organized in sub-folders per genre), two sets are defined\n",
        "# - The training test will be used to train the model, evaluate each execution and consequently update it\n",
        "# - The test set will be used to compute the final evaluation of the model\n",
        "# Since the dataset is almost perfectly balanced, the same structure is kept in both the training and the test sets\n",
        "# --> considering 8000 images (1000 per genre), the training set contains 80% of them (6400 images, 800 per genre),\n",
        "#     while the test set has the remaing 20% (1600 images, 200 per genre)\n",
        "\n",
        "# Output directories, aka where the images will be copied from \"dataset\"\n",
        "# (The dataset directory has been created in the previous cell)\n",
        "TRAIN_DIRECTORY = \"/content/dataset_train\"\n",
        "TEST_DIRECTORY  = \"/content/dataset_test\"\n",
        "os.makedirs(TRAIN_DIRECTORY, exist_ok = True)\n",
        "os.makedirs(TEST_DIRECTORY, exist_ok = True)\n",
        "\n",
        "# 1. Loading the images (spectrograms) and the labels (genres)\n",
        "\n",
        "file_paths = [] # it will contain all the images\n",
        "labels = []     # it will contain the corresponding labels\n",
        "\n",
        "genres = sorted(os.listdir(dataset_directory))\n",
        "\n",
        "for genre in genres:\n",
        "\n",
        "    genre_dir = os.path.join(dataset_directory, genre)\n",
        "    images = os.listdir(genre_dir) # all the images contained in a specific genre sub-folder (the one of the currente iteration)\n",
        "\n",
        "    for img in images:\n",
        "\n",
        "        file_paths.append(os.path.join(genre_dir, img)) # add the image to the container\n",
        "        labels.append(genre)                            # add the corresponding genre to the container\n",
        "\n",
        "file_paths = np.array(file_paths)\n",
        "labels = np.array(labels)\n",
        "\n",
        "# 2. Creating the split with \"StratifiedShuffleSplit()\", which automatically keeps the dataset balanced\n",
        "\n",
        "splitter = StratifiedShuffleSplit(n_splits = 1, test_size = 0.20, random_state = 42)\n",
        "\n",
        "for train_idx, test_idx in splitter.split(file_paths, labels):\n",
        "\n",
        "    train_files = file_paths[train_idx] # spectrograms of the trainig set\n",
        "    train_labels = labels[train_idx]    # genres of these spectrograms (training)\n",
        "    test_files  = file_paths[test_idx]  # spectrograms of the test set\n",
        "    test_labels = labels[test_idx]      # genres of these spectrograms (test)\n",
        "\n",
        "# 3. Copying the files in the corresponding folders (created at the beginning of this cell)\n",
        "\n",
        "def copy_files(files, labels, destination_root):\n",
        "    # Utility function to use to execute this last task\n",
        "    for file, label in zip(files, labels):\n",
        "        dest_dir = os.path.join(destination_root, label) # selecting the correct folder\n",
        "        os.makedirs(dest_dir, exist_ok = True)           # checking it exits\n",
        "        shutil.copy(file, dest_dir)                      # copying the image\n",
        "\n",
        "copy_files(train_files, train_labels, TRAIN_DIRECTORY)\n",
        "copy_files(test_files, test_labels, TEST_DIRECTORY)"
      ],
      "metadata": {
        "id": "4jX2ntHql2IF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# Ignoring the normalization temporarily in order to compute the mean and standard deviation of the training dataset\n",
        "# (See the following cell of code to understand the current)\n",
        "\n",
        "transform_no_norm = transforms.Compose([\n",
        "    transforms.ToTensor()])\n",
        "\n",
        "dataset_no_norm = datasets.ImageFolder(TRAIN_DIRECTORY, transform=transform_no_norm)\n",
        "\n",
        "loader = DataLoader(dataset_no_norm, batch_size = 64, shuffle = False, num_workers = 2)\n",
        "\n",
        "mean = 0.0\n",
        "std = 0.0\n",
        "total_images = 0\n",
        "\n",
        "for images, _ in loader:\n",
        "    # Structure of the images' shape: (batch, channels, H, W)\n",
        "    batch_samples = images.size(0)\n",
        "    images = images.view(batch_samples, images.size(1), -1)\n",
        "\n",
        "    mean += images.mean(2).sum(0)\n",
        "    std += images.std(2).sum(0)\n",
        "    total_images += batch_samples\n",
        "\n",
        "mean /= total_images\n",
        "std /= total_images\n",
        "\n",
        "print(\"Training set\\n\")\n",
        "print(\"Mean per channel =\", mean)\n",
        "print(\"Standard Deviation per channel =\", std)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "T2SG0vTU6OGV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# Ignoring the normalization temporarily in order to compute the mean and standard deviation of the test dataset\n",
        "# (See the following cell of code to understand the current)\n",
        "\n",
        "transform_no_norm = transforms.Compose([\n",
        "    transforms.ToTensor()])\n",
        "\n",
        "dataset_no_norm = datasets.ImageFolder(TEST_DIRECTORY, transform=transform_no_norm)\n",
        "\n",
        "loader = DataLoader(dataset_no_norm, batch_size = 64, shuffle = False, num_workers = 2)\n",
        "\n",
        "mean = 0.0\n",
        "std = 0.0\n",
        "total_images = 0\n",
        "\n",
        "for images, _ in loader:\n",
        "    # Structure of the images' shape: (batch, channels, H, W)\n",
        "    batch_samples = images.size(0)\n",
        "    images = images.view(batch_samples, images.size(1), -1)\n",
        "\n",
        "    mean += images.mean(2).sum(0)\n",
        "    std += images.std(2).sum(0)\n",
        "    total_images += batch_samples\n",
        "\n",
        "mean /= total_images\n",
        "std /= total_images\n",
        "\n",
        "print(\"Test set\\n\")\n",
        "print(\"Mean per channel =\", mean)\n",
        "print(\"Standard Deviation per channel =\", std)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "CJsLCSfot6qH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a dataset of images for PyTorch, and loading it in a DataLoader object\n",
        "# Doing so allows the dataset to be ready for the training of a CNN\n",
        "\n",
        "training_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),                   # Transform the dimension of every image in a standard format (224*224 in this case)\n",
        "    transforms.ToTensor(),                           # Trasform every image into a PyTorch tensor, mapping the values from [0, 255] to [0, 1]\n",
        "    transforms.Normalize((0.6556, 0.2476, 0.2829),   # Standardize the values using the training dataset's mean and std computed in the previous cells\n",
        "                         (0.2269, 0.1693, 0.1223))]) # The values are now mapped in [-1, 1]\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),                   # Transform the dimension of every image in a standard format (224*224 in this case)\n",
        "    transforms.ToTensor(),                           # Trasform every image into a PyTorch tensor, mapping the values from [0, 255] to [0, 1]\n",
        "    transforms.Normalize((0.6495, 0.2441, 0.2835),   # Standardize the values using the test dataset's mean and std computed in the previous cell\n",
        "                         (0.2280, 0.1685, 0.1227))]) # The values are now mapped in [-1, 1]\n",
        "\n",
        "# The dataset are now generated (the structure required by CNNs is verified)\n",
        "train_dataset = datasets.ImageFolder(TRAIN_DIRECTORY, transform = training_transform)\n",
        "test_dataset = datasets.ImageFolder(TEST_DIRECTORY, transform = test_transform)\n",
        "\n",
        "# Check if everything went good\n",
        "print(\"Training set ------------------------------\")\n",
        "print(\"\\nNumber of categories found:\", len(train_dataset.classes)) # they should be 8\n",
        "print(\"Total number of images uploaded:\", len(train_dataset))    # they should be 6397\n",
        "print(\"\\nTest set ----------------------------------\")\n",
        "print(\"\\nNumber of categories found:\", len(test_dataset.classes)) # they should be 8\n",
        "print(\"Total number of images uploaded:\", len(test_dataset))    # they should be 1600"
      ],
      "metadata": {
        "id": "ZHPTZD2B99C4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Convolutional Neural Network (CNN)\n",
        "\n",
        "class simple_SpectrogramCNN(nn.Module):\n",
        "\n",
        "    def __init__(self, num_classes):\n",
        "\n",
        "        super(simple_SpectrogramCNN, self).__init__()\n",
        "\n",
        "        # Define the Convolutional Layers\n",
        "\n",
        "        self.conv_layers = nn.Sequential(\n",
        "\n",
        "            # First Convolutional Layer\n",
        "            nn.Conv2d(3, 32, kernel_size = 5, stride = 1, padding = 1),\n",
        "            # Only in this initial layer the kernell is a 5*5 block instead of a 3*3, in order to capture more information from the spectrograms in input\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.LeakyReLU(negative_slope = 0.01),\n",
        "            # The activation function LeakyReLU() is preferred to ReLu() because it captures also negative values, instead of forcing them to zero\n",
        "            # (however, they have a very low weight, set to 0.01)\n",
        "            nn.Dropout2d(0.2), # Usually, the dropout rate lies in the interval [0.1, 0.3] for the Convolutional Layers,\n",
        "                               # and in [0.4, 0.6] for the Fully-connected Layers\n",
        "            nn.MaxPool2d(2),   # \"MaxPool2d()\" is implemented using 2*2 blocks, and preferred to \"AvgPool2d()\" in the case of image classification\n",
        "\n",
        "            # Second Convolutional Layer\n",
        "            nn.Conv2d(32, 64, kernel_size = 3, stride = 1, padding = 1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.LeakyReLU(negative_slope = 0.01),\n",
        "            nn.Dropout2d(0.2),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            # Third Convolutional Layer\n",
        "            nn.Conv2d(64, 128, kernel_size = 3, stride = 1, padding = 1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.GELU(),         # For this final Convolutional Layer, GELU() is preferred to ReLU(), whereas the latter will be used in the FC Layers, ...\n",
        "            nn.Dropout2d(0.3), # ... and the Dropout rate is increased by 0.1 to avoid overfitting\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            # Fourth Convolutional Layer\n",
        "            nn.Conv2d(128, 256, kernel_size = 3, stride = 1, padding = 1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout2d(0.3),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            nn.AdaptiveAvgPool2d((1,1)) # Reduce drastically the number of parameters, to avoid having an incredibly long vector with nn.Flatten()\n",
        "                                        # (With (2,2) or (3,3) as arguments, the reduction is less pronounced)\n",
        "        )\n",
        "\n",
        "        # Computing the output's dimension of the final Convolutional Layer\n",
        "        # It will be the input dimension of the first Linear Fully-connected Layer\n",
        "\n",
        "        input_shape = (3, 224, 224)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            dummy = torch.zeros(1, *input_shape)\n",
        "            conv_out = self.conv_layers(dummy)\n",
        "            conv_out_size = conv_out.view(1, -1).size(1)\n",
        "\n",
        "        # Define the Fully-connected Layers\n",
        "\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Flatten(),                  # Transform the feature maps of the final Convolutional Layer in a vector of C*H*W length\n",
        "            nn.Linear(conv_out_size, 256), # Reduce dimensionality from C*H*W to 256 with a linear transformation\n",
        "            nn.ReLU(),                     # ReLU is used as Activation Function\n",
        "            nn.Dropout(0.5),               # A \"bigger\" dropout is used to avoid overfitting\n",
        "            nn.Linear(256, num_classes)    # Reduce dimensionality from 256 to 8 (# categories) with a linear transformation\n",
        "            # The softmax function is not required since it's direclty implemented in the definition of the Loss function (CrossEntropyLoss, see the next cell)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_layers(x)\n",
        "        x = self.fc_layers(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "QVwbtkr6yMg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the number of parameters in the model\n",
        "\n",
        "# This function calculates the total number of trainable parameters in a PyTorch model.\n",
        "# It helps in understanding the complexity and capacity of different models.\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "CNN_model = simple_SpectrogramCNN(num_classes = 8)\n",
        "print(\"MLP Parameters:\", count_parameters(CNN_model))"
      ],
      "metadata": {
        "id": "gMGiUwG7xGIA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function that computes the necessary metrics to evaluate the model\n",
        "\n",
        "def model_metrics(model, dataloader, device, num_classes, top_k = 3):\n",
        "\n",
        "  # The function accepts as input:\n",
        "  # - the model (CNN) for which it will compute the metrics\n",
        "  # - a datset of the \"DataLoader\" type to use for the computation\n",
        "  # - a \"torch.device\" object (see the previous cell)\n",
        "  # - the number of categories\n",
        "  # - the value of k for the Top-k Accuracy (see the following code)\n",
        "\n",
        "  model.eval() # set the model in modality \"evaluation\"\n",
        "\n",
        "  # ------------------------------\n",
        "  # 1. Saving the labels, predictions and vectors of probabilities\n",
        "  # ------------------------------\n",
        "\n",
        "  all_labels = []        # it will contain the true values (labels indeed)\n",
        "  all_predictions = []   # it will contain all the predictions\n",
        "  all_probabilities = [] # it will contain the vector of probabilities for every image\n",
        "\n",
        "  with torch.no_grad(): # block the tracking of the gradients (they're not needed during the evaluation phase)\n",
        "\n",
        "    for X, y in dataloader: # \"X\" is a tensor containing the features, while \"y\" contains the labels (targets)\n",
        "\n",
        "      X, y = X.to(device), y.to(device) # to accelerate the computation\n",
        "\n",
        "      outputs = model(X)                        # For every image, generate the scores for every class, but ...\n",
        "      predictions = outputs.argmax(dim = 1)     # ... save only the top-score (the prediction)\n",
        "      probabilities = softmax(outputs, dim = 1) # Save the probabilities of belonging to every class for every image\n",
        "\n",
        "      # Adding the labels, predictions and probabilities to the corresponding containers ...\n",
        "      all_labels.extend(y.cpu().numpy())\n",
        "      all_predictions.extend(predictions.cpu().numpy())\n",
        "      all_probabilities.extend(probabilities.cpu().numpy())\n",
        "\n",
        "  # ... and converting them into NumPy arrays\n",
        "  all_labels = np.array(all_labels)\n",
        "  all_predictions = np.array(all_predictions)\n",
        "  all_probabilities = np.array(all_probabilities)\n",
        "\n",
        "  # ------------------------------\n",
        "  # 2. Computing the Top-3 Accuracy (The Accuracy is computed considering the first three more likely categories,\n",
        "  # not only the top-1, aka the prediction)\n",
        "  # ------------------------------\n",
        "\n",
        "  topk_correct = 0\n",
        "\n",
        "  for t, p in zip(all_labels, all_probabilities):\n",
        "        if t in np.argsort(p)[-top_k:]:\n",
        "          topk_correct += 1\n",
        "\n",
        "  topk_accuracy = topk_correct / len(all_labels)\n",
        "\n",
        "  # ------------------------------\n",
        "  # 3. Confusion Matrix\n",
        "  # ------------------------------\n",
        "\n",
        "  conf_matrix = confusion_matrix(all_labels, all_predictions, labels = np.arange(num_classes))\n",
        "\n",
        "  # ------------------------------\n",
        "  # 4. Per-class Metrics (\"classification_report()\" computes precision, recall and f1-score)\n",
        "  # ------------------------------\n",
        "\n",
        "  per_class = classification_report(\n",
        "        all_labels, all_predictions, output_dict = True, zero_division = 0)\n",
        "  # The default option returns a formatted string, that can't be used to do additional computations:\n",
        "  # setting \"output_dict\" to True returns a Dictionary instead, and thus solves the problem\n",
        "\n",
        "  # ------------------------------\n",
        "  # 5. Saving all the metrics in a Dictionary\n",
        "  # ------------------------------\n",
        "\n",
        "  metrics = {\n",
        "\n",
        "        # Overall metrics (between three different options to compute these metrics - macro, micro and weighted - the first one was selected,\n",
        "        # since it's generally better for balanced datasets, such as the one we're considering)\n",
        "        \"accuracy\": accuracy_score(all_labels, all_predictions),\n",
        "        \"precision\": precision_score(all_labels, all_predictions, average = \"macro\", zero_division = 0),\n",
        "        \"recall\": recall_score(all_labels, all_predictions, average = \"macro\", zero_division = 0),\n",
        "        \"f1-score\": f1_score(all_labels, all_predictions, average = \"macro\", zero_division = 0),\n",
        "\n",
        "        \"per_class\": per_class,\n",
        "        \"confusion_matrix\": conf_matrix,\n",
        "        \"topk_accuracy\": topk_accuracy,\n",
        "\n",
        "        \"y_true\": all_labels,\n",
        "        \"y_predictions\": all_predictions,\n",
        "        \"y_probabilities\": all_probabilities\n",
        "    }\n",
        "\n",
        "  return metrics"
      ],
      "metadata": {
        "id": "0SaVihaG8Fo5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To avoid a specific warning during the execution of the next cell\n",
        "torch._inductor.config.max_autotune = False"
      ],
      "metadata": {
        "id": "5IlKxos5EzpX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross-Validation is used to compute the metrics defined in the previous cell, together with many others\n",
        "\n",
        "num_classes = len(train_dataset.classes) # number of categories in the dataset\n",
        "batch_size = 128                         # number of images per batch (it will be used in the \"DataLoader()\" function)\n",
        "\n",
        "k_folds = 4\n",
        "# The \"KFold()\" function generates an object that automatically manages the Cross-Validation\n",
        "kfold = KFold(n_splits = k_folds, shuffle = True, random_state = 42)\n",
        "\n",
        "all_metrics = []       # it will contain the metrics computed for every fold through the \"model_metrics()\" function (list of 5 dictionaries)\n",
        "accuracy_per_fold = [] # it will contain the Accuracy computed for every fold (5-elements vector)\n",
        "\n",
        "global_confusion_matrix = np.zeros((num_classes, num_classes), dtype = int) # initialize the confusion matrix as populated only of zeros\n",
        "\n",
        "# As in the \"model_metrics()\" function:\n",
        "all_labels = []        # it will contain the true values (labels indeed)\n",
        "all_predictions = []   # it will contain all the predictions\n",
        "all_probabilities = [] # it will contain the vectors of probabilities\n",
        "\n",
        "histories = [] # it will contain the training loss and the validation loss for every fold\n",
        "\n",
        "# In order to speed up a little the training of the CNN\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "scaler = torch.amp.GradScaler('cuda')\n",
        "\n",
        "for fold, (train_idx, valid_idx) in enumerate(kfold.split(train_dataset)):\n",
        "\n",
        "    # Create the Training and Validation sets ...\n",
        "    train_subset = Subset(train_dataset, train_idx)\n",
        "    valid_subset = Subset(train_dataset, valid_idx)\n",
        "\n",
        "    # ... and transform them in a \"DataLoader\" object\n",
        "    train_loader = DataLoader(train_subset, batch_size = batch_size, shuffle = True,\n",
        "                              num_workers = 2, pin_memory = True, worker_init_fn = seed_worker, generator = g)\n",
        "    valid_loader = DataLoader(valid_subset, batch_size = batch_size, shuffle = False,\n",
        "                              num_workers = 2, pin_memory = True, worker_init_fn = seed_worker, generator = g)\n",
        "    # About the \"DataLoader()\" function:\n",
        "    # --> \"batch_size\": every batch will be formed by that number of images (a bigger value optimizes the CPU usage, until it explodes at least)\n",
        "    # --> \"shuffle\": if TRUE, the order will be shuffled for every epoch\n",
        "    # --> \"num_workes\": the execution will have a better CPU usage (in a \"local execution\", it can even be set to 4-8)\n",
        "    # --> \"pin_memory\": accelerates the transfer GPU - CPU\n",
        "\n",
        "    # Initialize the model (CNN)\n",
        "    model = simple_SpectrogramCNN(num_classes).to(device)\n",
        "    model = torch.compile(model)\n",
        "\n",
        "    criterion = torch.nn.CrossEntropyLoss()                                           # Loss function selected\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr = 0.001, weight_decay = 1e-6) # Optimization technique implemented\n",
        "    # (L2 regularization (through \"weight_decay\") is added to the optimization technique to prevent overfitting)\n",
        "\n",
        "    history = {\"train_loss\": [], \"valid_loss\": []} # it will contain the training loss and the validation loss of the current fold\n",
        "\n",
        "    # Training\n",
        "\n",
        "    epochs = 10\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "      model.train() # setting the model in \"training modality\"\n",
        "\n",
        "      train_loss = 0 # it will contain the cumulative sum of the current epoch's losses (one per batch)\n",
        "\n",
        "      for images, labels in train_loader:\n",
        "\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad() # cleaning the gradient values of the previous epoch: they're all set to zero\n",
        "\n",
        "        with torch.amp.autocast('cuda'): # accelerate the execution\n",
        "\n",
        "            outputs = model(images)           # The output is computed ...\n",
        "            loss = criterion(outputs, labels) # ... as well as the value of the loss\n",
        "\n",
        "        # Backpropagation is implemented ...\n",
        "        scaler.scale(loss).backward()\n",
        "        # ... and the CNN's parameters are updated accordingly\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        train_loss += loss.item() # Update the value with the current batch's loss\n",
        "\n",
        "      train_loss_epoch = train_loss / len(train_loader) # ... compute the mean training loss of the current epoch ...\n",
        "      history[\"train_loss\"].append(train_loss_epoch)    # ... and add it to the current fold's container\n",
        "\n",
        "      # The same reasoning can be used for the validation loss of the current epoch\n",
        "\n",
        "      model.eval() # setting the model in \"evaluation modality\"\n",
        "\n",
        "      valid_loss = 0\n",
        "\n",
        "      with torch.no_grad():\n",
        "\n",
        "        for images, labels in valid_loader:\n",
        "\n",
        "          images, labels = images.to(device), labels.to(device)\n",
        "          outputs = model(images)\n",
        "\n",
        "          loss = criterion(outputs, labels)\n",
        "          valid_loss += loss.item()\n",
        "\n",
        "      valid_loss_epoch = valid_loss / len(valid_loader)\n",
        "      history[\"valid_loss\"].append(valid_loss_epoch)\n",
        "\n",
        "    # Evaluation metrics\n",
        "\n",
        "    metrics = model_metrics(model, valid_loader, device, num_classes, top_k = 3) # Metrics of the current fold ...\n",
        "    all_metrics.append(metrics)                                                  # ... added to the general container\n",
        "\n",
        "    accuracy_per_fold.append(metrics[\"accuracy\"]) # save the accuracy of the current fold\n",
        "\n",
        "    global_confusion_matrix += metrics[\"confusion_matrix\"] # update the confusion matrix\n",
        "\n",
        "    histories.append(history) # add the loss values (training and validation) of the current fold to the general container\n",
        "\n",
        "    # Saving the labels, predictions and probabilities of the current fold\n",
        "    all_labels.extend(metrics[\"y_true\"])\n",
        "    all_predictions.extend(metrics[\"y_predictions\"])\n",
        "    all_probabilities.extend(metrics[\"y_probabilities\"])\n",
        "\n",
        "    print(f\"\\n✅ Training and Evaluation over Fold {fold+1}/{k_folds} completed\") # visual check"
      ],
      "metadata": {
        "id": "XkmpuGNpBYRd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Overall results\n",
        "\n",
        "print(\"Overall metrics over 5-folds Cross-Validation\\n\")\n",
        "\n",
        "# Utility function for computing the mean value of a certain metric\n",
        "def mean_metric(metric_name):\n",
        "    return np.mean([m[metric_name] for m in all_metrics])\n",
        "\n",
        "print(\"Accuracy in every fold:\")\n",
        "for i, acc in enumerate(accuracy_per_fold):\n",
        "    print(f\"Fold {i+1}: {acc:.4f}\")\n",
        "\n",
        "print(f\"\\nMean Accuracy: {mean_metric('accuracy'):.4f}\")\n",
        "print(f\"Top-3 Mean Accuracy: {mean_metric('topk_accuracy'):.4f}\")\n",
        "\n",
        "print(f\"\\nMean Precision: {mean_metric('precision'):.4f}\")\n",
        "print(f\"Mean Recall: {mean_metric('recall'):.4f}\")\n",
        "print(f\"Mean F1-score: {mean_metric('f1-score'):.4f}\")"
      ],
      "metadata": {
        "id": "Nhu2YE5yCetI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the aggregated (over the 5-folds) Confusion Matrix\n",
        "\n",
        "plt.figure(figsize = (8,6))\n",
        "sns.heatmap(global_confusion_matrix, annot = True, fmt = \"d\", cmap = \"Blues\")\n",
        "plt.title(\"Aggregated Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted values\")\n",
        "plt.ylabel(\"True values\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "elBwmyOEQCqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the Learning Curve \"Mean (over the 5-folds) Training Loss vs Epochs\"\n",
        "\n",
        "# Extracting the training loss for every fold ...\n",
        "all_train_losses = [h[\"train_loss\"] for h in histories]  # [list of lists]\n",
        "# ... and converting them into a NumPy array\n",
        "all_train_losses = np.array(all_train_losses)\n",
        "\n",
        "# Computing the mean value for every fold\n",
        "mean_train_loss = np.mean(all_train_losses, axis = 0)\n",
        "\n",
        "# Plotting the results\n",
        "plt.plot(mean_train_loss)\n",
        "plt.title(\"Mean Training Loss vs Epochs (over 5-folds CV)\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Mean Loss\")\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zp8hVHRPQe7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the Learning Curves \"Training Loss vs Validation Loss\" for every epoch in every fold\n",
        "\n",
        "# Layout settings\n",
        "fig, axes = plt.subplots(2, 2, figsize = (12, 10))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, h in enumerate(histories):\n",
        "    ax = axes[i]\n",
        "    ax.plot(h[\"train_loss\"], label=\"Training Loss\")\n",
        "    ax.plot(h[\"valid_loss\"], label=\"Validation Loss\")\n",
        "    ax.set_title(f\"Fold {i+1}\")\n",
        "    ax.set_xlabel(\"Epoch\")\n",
        "    ax.set_ylabel(\"Loss\")\n",
        "    ax.legend()\n",
        "    ax.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "h_k9ISdc24UP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the aggregated per-class metrics (precision, recall, f1-score)\n",
        "\n",
        "# Instead of considering the value computed in every fold (saved in \"all_metrics\") for these three metrics,\n",
        "# the latter are computed confronting all predictions and labels (7997*5)\n",
        "precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "    all_labels, all_predictions, average = None, labels = np.arange(num_classes))\n",
        "\n",
        "plt.figure(figsize = (10,5))\n",
        "plt.bar(np.arange(num_classes) - 0.2, precision, width = 0.2, label = \"Precision\")\n",
        "plt.bar(np.arange(num_classes), recall, width = 0.2, label = \"Recall\")\n",
        "plt.bar(np.arange(num_classes) + 0.2, f1, width = 0.2, label = \"F1-score\")\n",
        "plt.xticks(np.arange(num_classes))\n",
        "plt.legend()\n",
        "plt.title(\"Aggregated per-class metrics (over 5-folds CV)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1Qe2Nh72Q1Kc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the ROC Curve for every class\n",
        "\n",
        "y_true_bin = label_binarize(all_labels, classes = np.arange(num_classes))\n",
        "all_probabilities = np.array(all_probabilities)\n",
        "\n",
        "plt.figure(figsize = (10,7))\n",
        "\n",
        "for i in range(num_classes):\n",
        "    fpr, tpr, _ = roc_curve(y_true_bin[:,i], all_probabilities[:,i])\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    plt.plot(fpr, tpr, label = f\"Class {i} (AUC = {roc_auc:.4f})\")\n",
        "\n",
        "plt.plot([0,1], [0,1], '--')\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"Aggregated ROC curves One-vs-Rest (over a 5-fold CV)\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "L6v44BraRR7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# After training and evaluating the model on 80% of the data through Cross-Validation, this portion of the dataset\n",
        "# will be used one last time, in its entirety, to train the CNN (its \"optimized version\"). The metrics of the model\n",
        "# will be then computed on the test set (the remaining 20% of the data) as final evaluation\n",
        "\n",
        "# 1. Preparing the two sets\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size = 128, shuffle = True,\n",
        "                              num_workers = 2, pin_memory = True, worker_init_fn = seed_worker, generator = g)\n",
        "test_loader = DataLoader(test_dataset, batch_size = 128, shuffle = True,\n",
        "                              num_workers = 2, pin_memory = True, worker_init_fn = seed_worker, generator = g)\n",
        "\n",
        "# 2. Training and evaluating the model one last time\n",
        "\n",
        "epochs = 10\n",
        "all_losses = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    epoch_loss = {\"train_loss\": [], \"test_loss\": []}\n",
        "    train_loss = 0.0\n",
        "    test_loss = 0.0\n",
        "\n",
        "    # Training\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with torch.amp.autocast('cuda'):\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    train_loss_epoch = train_loss / len(train_loader)\n",
        "    epoch_loss[\"train_loss\"] = train_loss_epoch\n",
        "\n",
        "    # Evaluation\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for images, labels in test_loader:\n",
        "\n",
        "          images, labels = images.to(device), labels.to(device)\n",
        "          outputs = model(images)\n",
        "\n",
        "          loss = criterion(outputs, labels)\n",
        "          test_loss += loss.item()\n",
        "\n",
        "    test_loss_epoch = test_loss / len(test_loader)\n",
        "    epoch_loss[\"test_loss\"] = test_loss_epoch\n",
        "\n",
        "    all_losses.append(epoch_loss)"
      ],
      "metadata": {
        "id": "zZt7KyownghY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Computing the final Accuracy on the Test set\n",
        "\n",
        "model.eval()\n",
        "\n",
        "correct = sum(\n",
        "    (torch.max(model(images.to(device)).data, 1)[1] == labels.to(device)).sum().item()\n",
        "    for images, labels in test_loader)\n",
        "total = len(test_dataset)\n",
        "\n",
        "print(f\"Accuracy on the Test set: {correct / total:.4f}\")"
      ],
      "metadata": {
        "id": "vYsUazhwjyqx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the Learning Curve \"Training Loss vs Validation Loss\" for every epoch\n",
        "\n",
        "# Extracting the necessary values\n",
        "train_losses = [x[\"train_loss\"] for x in all_losses]\n",
        "test_losses = [x[\"test_loss\"] for x in all_losses]\n",
        "\n",
        "# Plotting the results\n",
        "plt.figure(figsize = (8,5))\n",
        "plt.plot(train_losses, label = \"Training Loss\", marker = 'o')\n",
        "plt.plot(test_losses, label = \"Test Loss\", marker = 'o')\n",
        "plt.title(\"Training Loss vs Test Loss per Epoch\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BIYPygDSj0y0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}